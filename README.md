 ğŸ“ˆ Linear Regression From Scratch with Streamlit Interface
# ğŸ“ˆ Linear Regression from Scratch with Streamlit

This repository implements **Linear Regression** using **Gradient Descent** from scratch in Python (no `sklearn`), with full **mathematical formulation**, an **interactive Streamlit UI**, and a focus on understanding core ML concepts visually and numerically.
This project demonstrates how to build **Linear Regression** from scratch using only **NumPy**, **Pandas**, and **Matplotlib** â€” without relying on any machine learning libraries like `scikit-learn`. It also features an interactive **Streamlit-based UI**, allowing users to upload their dataset, tweak hyperparameters like **learning rate** and **epochs**, and visually understand how gradient descent affects model learning.

---

## ğŸ¯ Objective

Given a dataset with features `x` and target `y`, we fit a straight line:

\[
y = mx + b
\]

The goal is to learn the **slope `m`** and **intercept `b`** such that the line best fits the data by minimizing prediction errors.
To fit a straight line `y = mx + b` that best approximates the data using **Gradient Descent**, minimizing the prediction error (Mean Squared Error).

---

## ğŸ§  Mathematical Foundations

### 1. **Prediction Function**

\[
\hat{y} = mx + b
\]

---
### 1. Prediction Function

### 2. **Loss Function: Mean Squared Error (MSE)**
Å· = mx + b

\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - (mx_i + b))^2
\]

Where:
- \( y_i \) = true value
- \( \hat{y}_i \) = predicted value
- \( n \) = number of samples
- `m` = slope (weight)
- `b` = intercept (bias)
- `x` = input feature
- `Å·` = predicted output

---

### 3. **Gradient Descent Optimization**
### 2. Loss Function: Mean Squared Error (MSE)

We want to minimize MSE by updating parameters in the **direction of steepest descent**:
MSE = (1/n) * Î£ (yáµ¢ - Å·áµ¢)Â²
= (1/n) * Î£ (yáµ¢ - (mxáµ¢ + b))Â²

**Gradient of MSE w.r.t slope `m`:**
\[
\frac{\partial \text{MSE}}{\partial m} = -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - (mx_i + b))
\]

**Gradient of MSE w.r.t intercept `b`:**
\[
\frac{\partial \text{MSE}}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (mx_i + b))
\]
Where:
- `yáµ¢` = true output
- `Å·áµ¢` = predicted output
- `n` = number of data points

---

### 4. **Parameter Updates**
### 3. Gradient Descent Optimization

Using a learning rate \( \alpha \), update rules:
To minimize the loss, we compute gradients:

\[
m := m - \alpha \cdot \frac{\partial \text{MSE}}{\partial m}
\]
\[
b := b - \alpha \cdot \frac{\partial \text{MSE}}{\partial b}
\]
**Gradient w.r.t. slope `m`:**

Repeat for `epochs` number of iterations.
âˆ‚MSE/âˆ‚m = (-2/n) * Î£ xáµ¢ * (yáµ¢ - (mxáµ¢ + b))

---

## ğŸ§¾ Code Architecture
**Gradient w.r.t. intercept `b`:**

- `LinearRegressionGD` â€“ custom class to:
  - Train using gradient descent
  - Predict new values
  - Track loss over time
âˆ‚MSE/âˆ‚b = (-2/n) * Î£ (yáµ¢ - (mxáµ¢ + b))

- `Streamlit UI` â€“ interactive tool to:
  - Upload datasets
  - Set learning rate and epochs
  - Visualize the regression line and loss curve

---

## ğŸ–¼ï¸ Visuals
### 4. Parameter Update Rules

Using learning rate `Î±`, we iteratively update:

m := m - Î± * âˆ‚MSE/âˆ‚m
b := b - Î± * âˆ‚MSE/âˆ‚b

### Regression Line:
- Scatter plot of data points
- Best-fit line dynamically drawn

### Loss Curve:
- X-axis: Epochs
- Y-axis: MSE Loss
Repeat for a fixed number of `epochs` or until the change in loss is below a threshold.

---

## ğŸ§ª Streamlit App: How to Use
## ğŸ§¾ Features

### 1. âœ… Setup Environment
- âœ… Linear regression from scratch using NumPy
- âœ… Manual implementation of gradient descent
- âœ… Dynamic control over learning rate and epochs via Streamlit
- âœ… Upload your own datasets with `x` and `y` columns
- âœ… Real-time visualization of regression line and loss curve

```bash
---

## ğŸ“¤ Sample CSV Format

Make sure your CSV has exactly two columns named `x` and `y`:

```csv
x,y
1,2
2,4
3,5.9
4,8
5,10
ğŸ“Š Visual Outputs

ğŸ“ Regression Line
Scatter plot of the data
Best-fit line using trained parameters
ğŸ“‰ Loss Curve
Tracks MSE across epochs
Shows how loss decreases with learning
ğŸ–¥ How to Run the App

1. Clone the Repository
git clone https://github.com/yaminiDKS/linear_regression_from_scratch.git
cd linear-regression-from-scratch
2. Create Virtual Environment (Optional but Recommended)
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
source venv/bin/activate      # On Windows: venv\Scripts\activate
3. Install Dependencies
pip install -r requirements.txt
4. Launch the Streamlit App
streamlit run linear_regression_streamlit.py
ğŸ§ª Project Structure

â”œâ”€â”€ linear_regression_streamlit.py     # Main Streamlit app
â”œâ”€â”€ data.csv                           # Sample dataset
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # This file


ğŸ“š Educational Value

This project is ideal for:

Beginners learning ML math
Anyone curious about how gradient descent really works
Students preparing for interviews
Recruiters assessing understanding beyond libraries
ğŸš€ Future Enhancements

 Multivariable linear regression
 Real-time animation of line updates
 Comparison with scikit-learn results
 Downloadable trained parameters